{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49ed6fa",
   "metadata": {},
   "source": [
    "# Bysykkel and Topography: Analyzing Oslo's Urban Bike Patterns\n",
    "This project explores how Oslo's elevation influences bike-sharing usage using data from the entire year. Enrich station data with elevation, calculate elevation differences and gradients per trip, and analyze usage trends.  \n",
    "  \n",
    "Data source: https://oslobysykkel.no/apne-data/historisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25411ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import glob\n",
    "import requests\n",
    "import os\n",
    "from haversine import haversine, Unit\n",
    "from datetime import datetime as dt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "\n",
    "fig_size = (16,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97011f",
   "metadata": {},
   "source": [
    "## Step 1 – Load Raw Trip Data into DuckDB\n",
    "\n",
    "This step reads all monthly CSV files from the `../data/` folder and loads them into a DuckDB database file (`db/bysykkel_2024.duckdb`). If the database doesn't exist, it will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSVs into DuckDB table 'trips_raw'\n",
    "con = duckdb.connect(\"../db/bysykkel_2024.duckdb\")\n",
    "\n",
    "csv_files = glob.glob(\"../data/??.csv\")\n",
    "for i, file in enumerate(csv_files):\n",
    "    if i == 0:\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE trips_raw AS SELECT * FROM read_csv_auto('{file}')\")\n",
    "    else:\n",
    "        con.execute(f\"INSERT INTO trips_raw SELECT * FROM read_csv_auto('{file}')\")\n",
    "\n",
    "con.execute(\"CHECKPOINT\")\n",
    "print(\"Loaded trips_raw data into DuckDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26951e",
   "metadata": {},
   "source": [
    "## Step 1.1 - Clean the dataset, explore columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = con.execute('SELECT * FROM trips_raw').df()\n",
    "trips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99089b8f",
   "metadata": {},
   "source": [
    "There are no nan values, except for in station descriptions. Remove the station descriptions, they are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = trips.drop(['start_station_description', 'end_station_description'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148d694",
   "metadata": {},
   "source": [
    "##### 1.1.1 Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "(trips['duration']/60).hist(bins=600)\n",
    "plt.xlim([0, 60])\n",
    "plt.xlabel('Ride duration [min]')\n",
    "plt.ylabel('Number of rides')\n",
    "plt.title('Histogram for the ride duration');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcecd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove trips that are longer than 2 hours \n",
    "trips = trips[trips['duration']<=7200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb0081",
   "metadata": {},
   "source": [
    "##### 1.1.2 started_at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48001174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Check if ended_at is always after started_at\n",
    "(trips['ended_at'] >= trips['started_at']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Group by day\n",
    "daily = trips.groupby(trips['started_at'].dt.date).size()\n",
    "\n",
    "# Convert index to datetime (from date)\n",
    "daily.index = pd.to_datetime(daily.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.plot(daily.index, daily.values, color='royalblue')\n",
    "\n",
    "# Format x-axis with months\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "plt.title(\"Daily Ride Count (Binned by Day)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Rides\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cc0ec",
   "metadata": {},
   "source": [
    "There are two peaks, one before and one right after the summer vacation in July."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031fe1ae",
   "metadata": {},
   "source": [
    "##### Investigate loops  \n",
    "Check how many rides had identical start and end stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loops = trips[trips['start_station_id']==trips['end_station_id']]\n",
    "len(df_loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352413d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "(df_loops['duration']/60).hist(bins=1000)\n",
    "plt.xlim([0, 5])\n",
    "plt.xlabel('Ride duration [min]')\n",
    "plt.ylabel('Number of rides')\n",
    "plt.title('Distribution of ride duration with identical start and end station');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66ade8",
   "metadata": {},
   "source": [
    "I want to investigate topology. Do loops even matter here? No. They don’t carry slope info. I will remove the useless ones and tag the rest.  \n",
    "  \n",
    "  Trips that started and ended at the same station with a duration under 3 minutes will be removed, as they likely represent cancelled or test rides. Remaining loops will be tagged as is_loop but excluded from slope-based analysis, as their paths and elevation data cannot be meaningfully inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop short loops under 3 minutes\n",
    "trips = trips[~((trips['start_station_id'] == trips['end_station_id']) & (trips['duration'] < 180))]\n",
    "\n",
    "# Tag remaining loops\n",
    "trips.loc[:,'is_loop'] = trips['start_station_id'] == trips['end_station_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot duration distribution, log scale helps\n",
    "plt.figure(figsize=fig_size)\n",
    "sns.histplot(data=trips, x='duration', hue='is_loop', bins=100, log_scale=True)\n",
    "plt.title(\"Ride Duration: Loops vs. Non-Loops\")\n",
    "plt.xlabel(\"Ride Duration [sec]\")\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned and tagged trips back to DuckDB\n",
    "con.register(\"cleaned_df\", trips)\n",
    "con.execute(\"CREATE OR REPLACE TABLE trips_v1_cleaned AS SELECT * FROM cleaned_df\")\n",
    "con.unregister(\"cleaned_df\")\n",
    "con.execute(\"CHECKPOINT\")\n",
    "print(\"Cleaned trips saved to DuckDB as 'trips_v1_cleaned'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76a271",
   "metadata": {},
   "source": [
    "## Step 2 – Extract Unique Stations\n",
    "\n",
    "Build a `stations` table by combining all distinct start and end stations from the `trips_cleaned` table. This gives the full list of physical bike stations to enrich with elevation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56516ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE stations AS\n",
    "SELECT DISTINCT\n",
    "    station_id,\n",
    "    station_name,\n",
    "    lat,\n",
    "    lon\n",
    "FROM (\n",
    "    SELECT\n",
    "        start_station_id AS station_id,\n",
    "        start_station_name AS station_name,\n",
    "        start_station_latitude AS lat,\n",
    "        start_station_longitude AS lon\n",
    "    FROM trips_v1_cleaned\n",
    "\n",
    "    UNION\n",
    "\n",
    "    SELECT\n",
    "        end_station_id AS station_id,\n",
    "        end_station_name AS station_name,\n",
    "        end_station_latitude AS lat,\n",
    "        end_station_longitude AS lon\n",
    "    FROM trips_v1_cleaned\n",
    ")\n",
    "ORDER BY station_id\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"CHECKPOINT\")\n",
    "print(\"Extracted and saved stations table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b8d37",
   "metadata": {},
   "source": [
    "## Step 3 - Using open-elevation api to populate the stations table with elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8421459",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../db/stations_with_elevation.csv\"\n",
    "\n",
    "def get_elevation(row):\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    \n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/elevation?latitude={lat}&longitude={lon}\")\n",
    "    data = response.json()\n",
    "    return data['elevation'][0]\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"Found existing data. Loading from csv.\")\n",
    "    stations = pd.read_csv(csv_path)\n",
    "    stations = stations.drop_duplicates(subset='station_id')\n",
    "else:\n",
    "    print(\"No csv found. Fetching elevation data from API.\")\n",
    "    stations = con.execute(\"SELECT * FROM stations\").df()\n",
    "    stations['elevation'] = stations.apply(get_elevation, axis=1)\n",
    "    stations = stations.drop_duplicates(subset='station_id')\n",
    "    stations.to_csv(\"../data/stations_with_elevation.csv\", index=False)\n",
    "\n",
    "print(\"Elevation received for all stations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86953a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save elevation data to database\n",
    "con.register(\"stations_df\", stations)\n",
    "con.execute(\"CREATE OR REPLACE TABLE stations AS SELECT * FROM stations_df\")\n",
    "con.unregister(\"stations_df\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join elevation data onto trips table\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE trips_v2_elevation AS\n",
    "SELECT\n",
    "    t.*,   \n",
    "    s_start.elevation AS start_elevation,\n",
    "    s_end.elevation AS end_elevation,\n",
    "    s_end.elevation - s_start.elevation AS elevation_diff\n",
    "FROM trips_v1_cleaned t\n",
    "JOIN stations s_start ON t.start_station_id = s_start.station_id\n",
    "JOIN stations s_end ON t.end_station_id = s_end.station_id\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"CHECKPOINT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SHOW TABLES\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = con.execute(\"SELECT * FROM trips_v2_elevation\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['elevation_diff'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['elevation_diff'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['elevation_diff'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef70c1f",
   "metadata": {},
   "source": [
    "## Step 4 - Compute travel distance and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(row):\n",
    "    start = (row['start_station_latitude'], row['start_station_longitude'])\n",
    "    end = (row['end_station_latitude'], row['end_station_longitude'])\n",
    "    return haversine(start, end, unit=Unit.METERS)\n",
    "\n",
    "trips[\"distance\"] = trips.apply(compute_distance, axis=1)\n",
    "\n",
    "def compute_gradient(row):\n",
    "    if row['distance'] == 0:\n",
    "        return np.nan\n",
    "    return row['elevation_diff'] / row['distance'] * 100\n",
    "\n",
    "trips[\"gradient\"] = trips.apply(compute_gradient, axis=1)\n",
    "\n",
    "# Update table with final fields\n",
    "con.register(\"stage_df\", trips)\n",
    "con.execute(\"CREATE OR REPLACE TABLE trips_v3_gradient AS SELECT * FROM stage_df\")\n",
    "con.unregister(\"stage_df\")\n",
    "con.execute(\"CHECKPOINT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SHOW TABLES\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = con.execute(\"FROM trips_v3_gradient SELECT *\").df()\n",
    "trips.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ca9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips[\"gradient\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips[\"gradient\"].hist(bins=200)\n",
    "plt.xlim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ich glaube sehr viele Leute fahren einfach am Meer entlang. Das ist sicher der Peak bei 0. wenn man den entfernt, dann sieht man vielleicht wie \"skewed\" es ist. Was ist wenn du die beliebtesten Routen ausfindig machst. Verusche ein Muster herauszufinen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['gradient'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_downhill = trips['gradient'][trips['gradient']<0].size\n",
    "n_uphill = trips['gradient'][trips['gradient']>0].size\n",
    "total_rides = trips['gradient'].size\n",
    "\n",
    "net_bias = (n_downhill - n_uphill) / total_rides * 100\n",
    "relative_increase = (n_downhill - n_uphill) / n_uphill * 100\n",
    "\n",
    "print(f\"Out of all rides, {net_bias:.0f}% more go downhill than uphill.\")\n",
    "print(f\"When comparing only uphill and downhill rides, downhill trips are {relative_increase:.0f}% more common than uphill ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037d7c7",
   "metadata": {},
   "source": [
    "- Computed elevation differece has a skew of -0.22 (mildy downhill-biased)\n",
    "- Elevation difference alone is misleading -> compute gradient\n",
    "- Found gradient skew is +0.10, but visually looks more negatively biased\n",
    "- Noticed a spike around 0-small positive gradient -> suspect: flat seaside rides\n",
    "- Have a sense that total ride time might be a more honest metric than ride count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb5830",
   "metadata": {},
   "source": [
    "## Step 5 - Use ride duration and total distance ridden as new metric\n",
    "Ride count can be misleading.  \n",
    "  \n",
    "Trying these alternate targets:  \n",
    "    - Total ride duration  \n",
    "    - Total distance ridden  \n",
    "\n",
    "Will do this for downhill vs uphill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slope type\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE trips_v4_slope AS\n",
    "SELECT *,\n",
    "    CASE\n",
    "        WHEN gradient IS NULL THEN 'unknown'\n",
    "        WHEN gradient < 0 THEN 'downhill'\n",
    "        WHEN gradient > 0 THEN 'uphill'\n",
    "        ELSE 'flat'\n",
    "    END AS slope_type\n",
    "FROM trips_v3_gradient\n",
    "\"\"\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SHOW TABLES\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Trip Duration, split by gradient sign\n",
    "\n",
    "trips = con.execute(\"SELECT * FROM trips_v4_slope\").df()\n",
    "trips = trips[~trips['is_loop']]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=fig_size)\n",
    "sns.histplot(data=trips, x='duration', hue='slope_type', hue_order=['flat', 'uphill', 'downhill'], bins=50, kde=True, log_scale=True)\n",
    "plt.title(\"Trip Duration Distribution by Gradient Direction\")\n",
    "plt.xlabel(\"Trip Duration (seconds)\")\n",
    "plt.ylabel(\"Number of Trips\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Trip Distance, split by gradient sign\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.histplot(data=trips, x='distance', hue='slope_type', hue_order=['flat', 'uphill', 'downhill'], bins=50, kde=True, log_scale=True)\n",
    "plt.title(\"Trip Distance Distribution by Gradient Direction\")\n",
    "plt.xlabel(\"Trip Distance (meters)\")\n",
    "plt.ylabel(\"Number of Trips\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = trips.groupby('slope_type').agg({\n",
    "    'duration': 'sum',\n",
    "    'distance': 'sum',\n",
    "    'gradient': 'mean'\n",
    "})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values\n",
    "duration_down = summary.loc['downhill', 'duration']\n",
    "duration_up = summary.loc['uphill', 'duration']\n",
    "distance_down = summary.loc['downhill', 'distance']\n",
    "distance_up = summary.loc['uphill', 'distance']\n",
    "\n",
    "# Relative increase (compared to uphill)\n",
    "duration_diff_pct = (duration_down - duration_up) / duration_up * 100\n",
    "distance_diff_pct = (distance_down - distance_up) / distance_up * 100\n",
    "\n",
    "# Net share difference (relative to total)\n",
    "duration_total = duration_down + duration_up\n",
    "distance_total = distance_down + distance_up\n",
    "\n",
    "duration_net_share = (duration_down - duration_up) / duration_total * 100\n",
    "distance_net_share = (distance_down - distance_up) / distance_total * 100\n",
    "\n",
    "print(f\"Duration:\")\n",
    "print(f\"• Downhill trips took {duration_diff_pct:.1f}% more total ride time than uphill trips.\")\n",
    "print(f\"• Net share: {duration_net_share:.1f}% more ride time downhill than uphill.\\n\")\n",
    "\n",
    "print(f\"Distance:\")\n",
    "print(f\"• Downhill trips covered {distance_diff_pct:.1f}% more total distance than uphill trips.\")\n",
    "print(f\"• Net share: {distance_net_share:.1f}% more distance ridden downhill than uphill.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average duration and distance per ride, grouped by slope\n",
    "avg_summary = trips.groupby('slope_type').agg({\n",
    "    'duration': ['mean', 'count'],\n",
    "    'distance': 'mean'\n",
    "})\n",
    "\n",
    "avg_summary.columns = ['avg_duration', 'ride_count', 'avg_distance']\n",
    "avg_summary = avg_summary.reset_index()\n",
    "\n",
    "print(\"Average Ride Metrics by Slope Type:\\n\")\n",
    "print(avg_summary.to_string(index=False))\n",
    "\n",
    "for _, row in avg_summary.iterrows():\n",
    "    slope = row['slope_type']\n",
    "    print(f\"• {slope.capitalize()} rides: average duration = {row['avg_duration']:.1f} seconds, \"\n",
    "          f\"average distance = {row['avg_distance']:.1f} meters \"\n",
    "          f\"({int(row['ride_count'])} rides)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b7c97",
   "metadata": {},
   "source": [
    "On average, uphill rides are 11% longer in time, despite covering ~9% less distance than downhill rides. This suggests that uphill rides are not only fewer, but also slower—consistent with the physical challenge of biking uphill. Downhill rides, on the other hand, are both more common and slightly longer in distance, indicating that gravity is a strong motivator for mobility behavior in Oslo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34f8f5",
   "metadata": {},
   "source": [
    "Interpretation  \n",
    "\t•\tMore trips go downhill than uphill  \n",
    "\t•\tDownhill trips take more time in total, even though each is typically shorter, suggesting they’re more common  \n",
    "\t•\tUphill trips are longer per ride, because it takes more effort  \n",
    "\t•\tThis confirms that elevation influences mobility patterns  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4c08e",
   "metadata": {},
   "source": [
    "## Step 6 - Spatial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bd6ed",
   "metadata": {},
   "source": [
    "### 6.1 Investigate which stations are importers and exporters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_out = trips.groupby('start_station_id').size().rename('departures')\n",
    "flux_in = trips.groupby('end_station_id').size().rename('arrivals')\n",
    "\n",
    "flux = flux_out.to_frame().join(flux_in.to_frame(), how='outer').fillna(0)\n",
    "flux['net_flux'] = flux['arrivals'] - flux['departures']\n",
    "flux['total_usage'] = flux['arrivals'] + flux['departures']\n",
    "flux['departure_share'] = flux['departures'] / flux['total_usage']\n",
    "\n",
    "flux = stations.join(flux, how=\"outer\", on=\"station_id\").fillna(0)\n",
    "\n",
    "# Update table with final fields\n",
    "con.register(\"stage_df\", flux)\n",
    "con.execute(\"CREATE OR REPLACE TABLE flux AS SELECT * FROM stage_df\")\n",
    "con.unregister(\"stage_df\")\n",
    "con.execute(\"CHECKPOINT\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b25e28",
   "metadata": {},
   "source": [
    "#### 6.1.1 Elevation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive Folium map of Oslo to show elevation and usage of bike stations\n",
    "\n",
    "oslo_coordinates = [59.9139, 10.7522]\n",
    "\n",
    "import branca.colormap as cm\n",
    "\n",
    "elevation_min = stations['elevation'].min()\n",
    "elevation_max = stations['elevation'].max()\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['blue', 'yellow', 'red'],\n",
    "    vmin=elevation_min,\n",
    "    vmax=elevation_max\n",
    ")\n",
    "\n",
    "def get_radius(total_usage):\n",
    "    # Normalize usage\n",
    "    return np.interp(total_usage,\n",
    "                     (flux['total_usage'].min(), flux['total_usage'].max()),\n",
    "                     (4, 17))\n",
    "    \n",
    "elevation_map = folium.Map(location=oslo_coordinates, zoom_start=13)\n",
    "\n",
    "# Print all stations onto the map\n",
    "for index, row in flux.iterrows():\n",
    "\n",
    "    popup_content = f\"\"\"\n",
    "    Station: {row['station_name']} <br>\n",
    "    Elevation: {row['elevation']:.0f}m <br>\n",
    "    Departures: {row['departures']} <br>\n",
    "    Arrivals: {row['arrivals']} <br>\n",
    "    Total Usage: {row['total_usage']}\n",
    "    \"\"\"\n",
    "\n",
    "    # Add marker\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=get_radius(row['total_usage']),\n",
    "        color=colormap(row['elevation']),\n",
    "        fill=True,\n",
    "        fill_opacity=0.8,\n",
    "        weight=1,\n",
    "        popup=folium.Popup(popup_content, max_width=300)\n",
    "    ).add_to(elevation_map)\n",
    "\n",
    "elevation_map.save('../outputs/oslo_elevation_map.html')\n",
    "\n",
    "elevation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f25db",
   "metadata": {},
   "source": [
    "#### 6.1.2 Flux Map (importer and exporter stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive Folium map of Oslo to spatially demonstrate the behaviour where bikes are flowing\n",
    "\n",
    "oslo_coordinates = [59.9139, 10.7522]\n",
    "\n",
    "def get_color(flux_value):\n",
    "    if flux_value > 1000:\n",
    "        return '#1a9850' #'darkgreen'\n",
    "    elif flux_value > 0:\n",
    "        return 'yellowgreen' \n",
    "    elif flux_value > -1000:\n",
    "        return '#fc8d59' #'orange'\n",
    "    else:\n",
    "        return '#d73027' #'red'\n",
    "    \n",
    "def get_radius(total_usage):\n",
    "    # Normalize usage\n",
    "    return np.interp(total_usage,\n",
    "                     (flux['total_usage'].min(), flux['total_usage'].max()),\n",
    "                     (2, 17))\n",
    "\n",
    "flux_map = folium.Map(location=oslo_coordinates, zoom_start=13)\n",
    "\n",
    "# Print all stations onto the map\n",
    "for index, row in flux.iterrows():\n",
    "\n",
    "    popup_content = f\"\"\"\n",
    "    Station: {row['station_name']} <br>\n",
    "    Elevation: {row['elevation']:.0f}m <br>\n",
    "    Departures: {row['departures']} <br>\n",
    "    Arrivals: {row['arrivals']} <br>\n",
    "    Total Usage: {row['total_usage']} <br>\n",
    "    Net Flux: {row['net_flux']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add marker\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=get_radius(row['total_usage']),\n",
    "        color=get_color(row['net_flux']),\n",
    "        fill=True,\n",
    "        fill_color=get_color(row['net_flux']),\n",
    "        fill_opacity=0.8,\n",
    "        weight=1,\n",
    "        popup=folium.Popup(popup_content, max_width=300)\n",
    "    ).add_to(flux_map)\n",
    "\n",
    "# ToDo: Fix legend! Does not work\n",
    "\n",
    "flux_map.save('../outputs/oslo_flux_map.html')\n",
    "\n",
    "\n",
    "flux_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee612f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sorted net flux and elevation\n",
    "\n",
    "sorted_flux = flux.sort_values('net_flux')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "ax1.plot(range(len(sorted_flux)), sorted_flux['net_flux'], 'b-', linewidth=2, label='Net Flux')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create secondary axis for elevation\n",
    "ax2 = ax1.twinx()\n",
    "scatter = ax2.scatter(range(len(sorted_flux)), \n",
    "            sorted_flux['elevation'], \n",
    "            c=sorted_flux['elevation'], \n",
    "            cmap='YlOrRd', \n",
    "            alpha=0.7, \n",
    "            s=50, \n",
    "            edgecolor='k', \n",
    "            linewidth=0.5, \n",
    "            label='Elevation')\n",
    "\n",
    "# Colorbar for elevation\n",
    "cbar = plt.colorbar(scatter, pad=0.01)\n",
    "cbar.set_label('Elevation (m)', rotation=270, labelpad=20)\n",
    "\n",
    "# Set labels and title\n",
    "ax1.set_xlabel('Stations (Sorted by Net Flux)', fontsize=12)\n",
    "ax1.set_ylabel('Net Flux (Arrivals - Departures)', color='b', fontsize=12)\n",
    "plt.title('Station Net Flux vs. Elevation', fontsize=14)\n",
    "ax1.tick_params(axis='y', colors='blue')\n",
    "\n",
    "# Add subset of stations to x-axis\n",
    "step = 5\n",
    "ax1.set_xticks(range(0, len(sorted_flux), step))\n",
    "ax1.set_xticklabels(sorted_flux['station_name'].iloc[::step], rotation=90, ha='center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux.plot(kind=\"scatter\", x='elevation', y='net_flux', figsize=fig_size)\n",
    "plt.show()\n",
    "plt.savefig('../outputs/scatter_elev_flux.png')\n",
    "flux[['elevation', 'net_flux']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b5950",
   "metadata": {},
   "source": [
    "### 6.2 Direct Route Analysis: Compute most popular routes and most popular connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = con.execute('SELECT * FROM trips_v4_slope').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed routes\n",
    "trips[\"route\"] = trips[\"start_station_id\"].astype(str) + \"->\" + trips[\"end_station_id\"].astype(str)\n",
    "trips['route'] = trips['route'].astype('category')\n",
    "\n",
    "\n",
    "# Unidirectional route (connection)\n",
    "trips[\"connection\"] = trips.apply(\n",
    "    lambda row: f\"{min(row['start_station_id'], row['end_station_id'])}<->{max(row['start_station_id'], row['end_station_id'])}\",\n",
    "    axis=1\n",
    ")\n",
    "trips['connection'] = trips['connection'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b52e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database getting too large. Dropping old tables\n",
    "tables_to_drop = ['trips_v1_cleaned', 'trips_v2_elevation', 'trips_v3_gradient']\n",
    "for table in tables_to_drop:\n",
    "    con.execute(f\"DROP TABLE IF EXISTS {table}\")\n",
    "\n",
    "con.execute(\"VACUUM\")\n",
    "\n",
    "# # Save final trips table\n",
    "con.register(\"stage_df\", trips)\n",
    "con.execute(\"CREATE OR REPLACE TABLE trips_v5_routes AS SELECT * FROM stage_df\")\n",
    "con.unregister(\"stage_df\")\n",
    "con.execute(\"CHECKPOINT\")\n",
    "con.execute(\"SHOW TABLES\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe846c3d",
   "metadata": {},
   "source": [
    "#### 6.2.1 Compute stats and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_routes = 30\n",
    "n_connections = 100\n",
    "\n",
    "popular_routes = trips.groupby('route').size().reset_index(name='count').sort_values(by='count', ascending=False)\n",
    "popular_connections = trips.groupby('connection').size().reset_index(name='count').sort_values(by='count', ascending=False)\n",
    "\n",
    "top_routes = popular_routes.reset_index(drop=True).head(n_routes)\n",
    "top_connections = popular_connections.reset_index(drop=True).head(n_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich top routes and connections with station info\n",
    "station_lookup = stations.set_index('station_id')\n",
    "\n",
    "def get_station_info(row, column, delimiter):\n",
    "    start_id, end_id = row[column].split(delimiter)\n",
    "\n",
    "    start_station = station_lookup.loc[int(start_id), 'station_name']\n",
    "    end_station = station_lookup.loc[int(end_id), 'station_name']\n",
    "\n",
    "    start_elevation = station_lookup.loc[int(start_id), 'elevation']\n",
    "    end_elevation = station_lookup.loc[int(end_id), 'elevation']\n",
    "\n",
    "    start_lat = station_lookup.loc[int(start_id), 'lat']\n",
    "    end_lat = station_lookup.loc[int(end_id), 'lat']\n",
    "\n",
    "    start_lon = station_lookup.loc[int(start_id), 'lon']\n",
    "    end_lon = station_lookup.loc[int(end_id), 'lon']\n",
    "\n",
    "    return pd.Series({'start_station_id': start_id,\n",
    "                      'end_station_id': end_id,\n",
    "                      'start_station': start_station,\n",
    "                      'end_station': end_station,\n",
    "                      'start_elevation': start_elevation,\n",
    "                      'end_elevation': end_elevation,\n",
    "                      'start_lat': start_lat,\n",
    "                      'end_lat': end_lat,\n",
    "                      'start_lon': start_lon,\n",
    "                      'end_lon': end_lon})\n",
    "\n",
    "route_info = top_routes.apply(lambda row: get_station_info(row, 'route', '->'), axis=1)\n",
    "top_routes = pd.concat([top_routes, route_info], axis=1)\n",
    "\n",
    "connection_info = top_connections.apply(lambda row: get_station_info(row, 'connection', '<->'), axis=1)\n",
    "top_connections = pd.concat([top_connections, connection_info], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49692c67",
   "metadata": {},
   "source": [
    "#### 6.2.3 Visualize with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive Folium map of Oslo to show most popular bike routes\n",
    "\n",
    "oslo_coordinates = [59.9139, 10.7522]\n",
    "\n",
    "import branca.colormap as cm\n",
    "\n",
    "color_scale = cm.LinearColormap(['lightblue', 'blue', 'darkblue'],\n",
    "                                 vmin=top_connections['count'].min(),\n",
    "                                 vmax=top_connections['count'].max())\n",
    "\n",
    "connection_map = folium.Map(location=oslo_coordinates, zoom_start=13, tiles='CartoDB positron')\n",
    "\n",
    "def get_weight(count):\n",
    "    return np.interp(count,\n",
    "                    (top_connections['count'].min(), top_connections['count'].max()),\n",
    "                    (2, 20))\n",
    "\n",
    "\n",
    "\n",
    "# Print all bike routes\n",
    "for index, row in top_connections.iterrows():\n",
    "    \n",
    "    popup_content = f\"\"\"\n",
    "    {row['start_station']} → {row['end_station']}<br>\n",
    "    Count: {row['count']}\n",
    "    \"\"\"\n",
    "\n",
    "    folium.PolyLine(\n",
    "        locations=[[row['start_lat'], row['start_lon']],\n",
    "                   [row['end_lat'], row['end_lon']]],\n",
    "                   weight=get_weight(row['count']),\n",
    "                   opacity=0.5,\n",
    "                   color=color_scale(row['count']),\n",
    "                   popup=folium.Popup(popup_content, max_width=300),\n",
    "                    ).add_to(connection_map)\n",
    "    \n",
    "for index, row in stations.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        color='grey',\n",
    "        radius=2,\n",
    "        fill=True,\n",
    "        fill_opacity=0.8,\n",
    "        weight=2,\n",
    "        ).add_to(connection_map)\n",
    "    \n",
    "connection_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997c450",
   "metadata": {},
   "source": [
    "Here it's visible what the most important routes are. But hitting the ceiling of what this approach can do, will get chaotic very soon. Need to find pattern, which live on a higher level. Need to cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd9d2a",
   "metadata": {},
   "source": [
    "### 6.3 Cluster Analysis (DBscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a512570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "coordinates = stations[['lat', 'lon']]\n",
    "\n",
    "# Normalize coordinates such that latitude and longitude use the same scale\n",
    "scaler = StandardScaler()\n",
    "coords_scaled = scaler.fit_transform(coordinates)\n",
    "\n",
    "# Perform DBSCAN\n",
    "db = DBSCAN(eps=0.14, min_samples=2).fit(coords_scaled)\n",
    "stations['cluster_id'] = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d63f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assign colors\n",
    "unique_clusters = sorted(stations['cluster_id'].unique())\n",
    "color_map = {cluster: mcolors.to_hex(cm.tab20(i % 20)) for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# Create map\n",
    "cluster_map = folium.Map(location=oslo_coordinates, zoom_start=13,  tiles='CartoDB positron')\n",
    "\n",
    "for _, row in stations.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=5,\n",
    "        color=color_map[row['cluster_id']] if row['cluster_id'] != -1 else 'gray',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Station: {row['station_name']}<br>Cluster: {row['cluster_id']}\"\n",
    "    ).add_to(cluster_map)\n",
    "cluster_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99524b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster coordinates (centroids)\n",
    "clusters = stations[stations['cluster_id']>=0].groupby('cluster_id').agg({'lat': 'mean', 'lon': 'mean', 'elevation': 'mean', 'station_name': 'count'})\n",
    "clusters = clusters.reset_index()\n",
    "clusters.rename(columns={'lat': 'cluster_lat', 'lon': 'cluster_lon', 'elevation': 'cluster_elevation', 'station_name': 'cluster_size'}, inplace=True)\n",
    "\n",
    "# Enrich top_connections with start_cluster_id and end_cluster_id\n",
    "cluster_dict = stations.set_index('station_id')['cluster_id'].to_dict()\n",
    "\n",
    "top_connections['start_cluster'] = top_connections['start_station_id'].astype(int).map(cluster_dict)\n",
    "top_connections['end_cluster'] = top_connections['end_station_id'].astype(int).map(cluster_dict)\n",
    "\n",
    "# Drop rides where both start and end station are cluster noise\n",
    "top_connections = top_connections[~((top_connections['start_cluster']==-1) & (top_connections['end_cluster']==-1))]\n",
    "\n",
    "# Add centroid coordinates to top connections\n",
    "top_connections = top_connections.merge(\n",
    "    clusters[['cluster_id', 'cluster_lat', 'cluster_lon']].add_prefix('start_'),\n",
    "    left_on='start_cluster',\n",
    "    right_on='start_cluster_id',\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "top_connections = top_connections.merge(\n",
    "    clusters[['cluster_id', 'cluster_lat', 'cluster_lon']].add_prefix('end_'),\n",
    "    left_on='end_cluster',\n",
    "    right_on='end_cluster_id',\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# Treat noise clusters \n",
    "mask = top_connections['start_cluster'] == -1\n",
    "top_connections.loc[mask, 'start_cluster_lat'] = top_connections.loc[mask, 'start_lat']\n",
    "top_connections.loc[mask, 'start_cluster_lon'] = top_connections.loc[mask, 'start_lon']\n",
    "mask = top_connections['end_cluster'] == -1\n",
    "top_connections.loc[mask, 'end_cluster_lat'] = top_connections.loc[mask, 'end_lat']\n",
    "top_connections.loc[mask, 'end_cluster_lon'] = top_connections.loc[mask, 'end_lon']\n",
    "top_connections.drop(columns=['start_cluster_id', 'end_cluster_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top connections between clusters\n",
    "top_connections['cluster_a'] = top_connections[['start_cluster', 'end_cluster']].min(axis=1)\n",
    "top_connections['cluster_b'] = top_connections[['start_cluster', 'end_cluster']].max(axis=1)\n",
    "top_cluster_connections = top_connections.groupby(['cluster_a', 'cluster_b']).agg({\n",
    "    'count': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "# Add cluster info (lat, lon) to top cluster connections\n",
    "top_cluster_connections = top_cluster_connections.merge(\n",
    "    clusters[['cluster_id', 'cluster_lat', 'cluster_lon']],\n",
    "    left_on='cluster_a',\n",
    "    right_on='cluster_id',\n",
    "    how='left'\n",
    ").rename(columns={\n",
    "    'cluster_lat': 'start_lat',\n",
    "    'cluster_lon': 'start_lon'\n",
    "}).drop(columns='cluster_id')\n",
    "\n",
    "top_cluster_connections = top_cluster_connections.merge(\n",
    "    clusters[['cluster_id', 'cluster_lat', 'cluster_lon']],\n",
    "    left_on='cluster_b',\n",
    "    right_on='cluster_id',\n",
    "    how='left'\n",
    ").rename(columns={\n",
    "    'cluster_lat': 'end_lat',\n",
    "    'cluster_lon': 'end_lon'\n",
    "}).drop(columns='cluster_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing coordinates for noise cluster (-1) cases by looking up from top_connections\n",
    "def get_cluster_coordinates(row, connections_df):\n",
    "    cluster_a = row['cluster_a']\n",
    "    cluster_b = row['cluster_b']\n",
    "\n",
    "    matching_rows = connections_df[\n",
    "        ((connections_df['start_cluster'] == cluster_a) & (connections_df['end_cluster'] == cluster_b)) |\n",
    "        ((connections_df['start_cluster'] == cluster_b) & (connections_df['end_cluster'] == cluster_a))\n",
    "    ]\n",
    "\n",
    "    if matching_rows.empty:\n",
    "        return pd.Series({'start_lat': None, 'start_lon': None, 'end_lat': None, 'end_lon': None})\n",
    "\n",
    "    sample = matching_rows.iloc[0]\n",
    "\n",
    "    # Resolve A\n",
    "    if cluster_a == -1:\n",
    "        a_lat = sample['start_lat'] if sample['start_cluster'] == cluster_a else sample['end_lat']\n",
    "        a_lon = sample['start_lon'] if sample['start_cluster'] == cluster_a else sample['end_lon']\n",
    "    else:\n",
    "        a_lat = clusters[clusters['cluster_id'] == cluster_a]['cluster_lat'].values[0]\n",
    "        a_lon = clusters[clusters['cluster_id'] == cluster_a]['cluster_lon'].values[0]\n",
    "\n",
    "    # Resolve B\n",
    "    if cluster_b == -1:\n",
    "        b_lat = sample['start_lat'] if sample['start_cluster'] == cluster_b else sample['end_lat']\n",
    "        b_lon = sample['start_lon'] if sample['start_cluster'] == cluster_b else sample['end_lon']\n",
    "    else:\n",
    "        b_lat = clusters[clusters['cluster_id'] == cluster_b]['cluster_lat'].values[0]\n",
    "        b_lon = clusters[clusters['cluster_id'] == cluster_b]['cluster_lon'].values[0]\n",
    "\n",
    "    return pd.Series({\n",
    "        'start_lat': a_lat,\n",
    "        'start_lon': a_lon,\n",
    "        'end_lat': b_lat,\n",
    "        'end_lon': b_lon\n",
    "    })\n",
    "\n",
    "# Apply to fix missing coordinates\n",
    "fixed_coords = top_cluster_connections.apply(lambda row: get_cluster_coordinates(row, top_connections), axis=1)\n",
    "\n",
    "# Merge back\n",
    "top_cluster_connections[['start_lat', 'start_lon', 'end_lat', 'end_lon']] = fixed_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e62bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cluster_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19af339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive Folium map of Oslo to show flow between clusters\n",
    "\n",
    "oslo_coordinates = [59.9139, 10.7522]\n",
    "\n",
    "cluster_connection_map = folium.Map(location=oslo_coordinates, zoom_start=13,  tiles='CartoDB positron')\n",
    "\n",
    "import branca.colormap as cm\n",
    "\n",
    "color_scale = cm.LinearColormap(['lightblue', 'blue', 'darkblue'],\n",
    "                                 vmin=top_cluster_connections['count'].min(),\n",
    "                                 vmax=top_cluster_connections['count'].max())\n",
    "\n",
    "\n",
    "def get_weight(count):\n",
    "    return np.interp(count,\n",
    "                    (top_cluster_connections['count'].min(), top_cluster_connections['count'].max()),\n",
    "                    (2, 20))\n",
    "\n",
    "for _, row in stations.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=5,\n",
    "        color=color_map[row['cluster_id']] if row['cluster_id'] != -1 else 'lightgray',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Station: {row['station_name']}<br>Cluster: {row['cluster_id']}\"\n",
    "    ).add_to(cluster_connection_map)\n",
    "\n",
    "# Print all bike routes\n",
    "for index, row in top_cluster_connections.iterrows():\n",
    "    \n",
    "    # popup_content = f\"\"\"\n",
    "    # {row['start_station']} → {row['end_station']}<br>\n",
    "    # Count: {row['count']}\n",
    "    # \"\"\"\n",
    "\n",
    "    folium.PolyLine(\n",
    "        locations=[[row['start_lat'], row['start_lon']],\n",
    "                   [row['end_lat'], row['end_lon']]],\n",
    "                   weight=get_weight(row['count']),\n",
    "                   opacity=0.5,\n",
    "                   color=color_scale(row['count']),\n",
    "                #    popup=folium.Popup(popup_content, max_width=300),\n",
    "                    ).add_to(cluster_connection_map)\n",
    "\n",
    "used_clusters = set(top_cluster_connections['cluster_a']).union(\n",
    "                 set(top_cluster_connections['cluster_b']))\n",
    "\n",
    "active_clusters = clusters[clusters['cluster_id'].isin(used_clusters)]\n",
    "\n",
    "for _, row in active_clusters.iterrows():\n",
    "    cid = row['cluster_id']\n",
    "    folium.CircleMarker(\n",
    "        location=[row['cluster_lat'], row['cluster_lon']],\n",
    "        radius=10,\n",
    "        color=color_map[cid],\n",
    "        fill=False,\n",
    "        fill_color=color_map[cid],\n",
    "        fill_opacity=0.4,\n",
    "        weight=2,\n",
    "        popup=f\"Cluster {cid}<br>Size: {row['cluster_size']}\"\n",
    "    ).add_to(cluster_connection_map)\n",
    "    \n",
    "\n",
    "    \n",
    "cluster_connection_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79670c8a",
   "metadata": {},
   "source": [
    "### 6.4 Structural Analysis\n",
    "\n",
    "#### 6.4.1 Network Analysis with NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create directional graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add stations as nodes to graph\n",
    "for _, station in stations.iterrows():\n",
    "    G.add_node(station['station_id'],\n",
    "               name=station['station_name'],\n",
    "               lat=station['lat'],\n",
    "               lon=station['lon'],\n",
    "               elevation=station['elevation'])\n",
    "\n",
    "# Add trips between stations as edges, with frequency of route as weight\n",
    "trip_counts = trips.groupby(['start_station_id', 'end_station_id']).size().reset_index(name='weight')\n",
    "trip_counts = trip_counts.sort_values('weight', ascending=False)#.head(100)\n",
    "\n",
    "for _, trip in trip_counts.iterrows():\n",
    "    G.add_edge(trip['start_station_id'],\n",
    "               trip['end_station_id'],\n",
    "               weight=trip['weight'])\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "importers = dict(G.in_degree(weight='weight'))\n",
    "exporters = dict(G.out_degree(weight='weight'))\n",
    "total_activity = dict(G.degree(weight='weight'))\n",
    "betweenness = nx.betweenness_centrality(G, weight='weight') # How often a node sits on the shortest path between other nodes. Acts as a bridge\n",
    "pagerank = nx.pagerank(G, weight='weight') # Network hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network in Folium\n",
    "n_edges = 1000\n",
    "\n",
    "network_map = folium.Map(location=oslo_coordinates, zoom_start=13, tiles='CartoDB positron')\n",
    "\n",
    "# Add edges to map\n",
    "top_edges = sorted(\n",
    "    G.edges(data=True),\n",
    "    key=lambda x: x[2]['weight'],\n",
    "    reverse=True\n",
    "    )[:n_edges]\n",
    "\n",
    "# Compute weights for scaling\n",
    "weights = [w['weight'] for _, _, w in top_edges]\n",
    "min_weight, max_weight = min(weights), max(weights)\n",
    "\n",
    "colormap = cm.LinearColormap(['blue', 'darkblue', 'navy'],\n",
    "                              vmin=min_weight, vmax=max_weight)\n",
    "\n",
    "def scale(weight, val_min, val_max, scale_min, scale_max):\n",
    "    return np.interp(\n",
    "        weight,\n",
    "        (val_min, val_max),\n",
    "        (scale_min, scale_max))\n",
    "\n",
    "    \n",
    "for e1, e2, data in top_edges:\n",
    "    lat1, lon1 = G.nodes[e1]['lat'], G.nodes[e1]['lon']\n",
    "    lat2, lon2 = G.nodes[e2]['lat'], G.nodes[e2]['lon']\n",
    "    thickness = scale(data['weight'], min_weight, max_weight, 1, 26)\n",
    "\n",
    "    folium.PolyLine(\n",
    "        locations=[[lat1, lon1], [lat2, lon2]],\n",
    "        weight=thickness,\n",
    "        opacity=0.1,#scale(data['weight'], min_weight, max_weight, 0.3, 0.8),\n",
    "        color=colormap(data['weight']),#'blue',\n",
    "        dash_array='5, 5' if thickness < 2 else None\n",
    "        \n",
    "    ).add_to(network_map)\n",
    "\n",
    "\n",
    "# Add nodes to map\n",
    "for node, data in G.nodes(data=True):\n",
    "    popup_content = \"popup\"\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        location=[data['lat'], data['lon']],\n",
    "        color='gray',\n",
    "        radius=2,\n",
    "        fill=True,\n",
    "        fill_opacity=0.3,\n",
    "        popup=popup_content\n",
    "    ).add_to(network_map)\n",
    "\n",
    "\n",
    "# Visualize top hubs\n",
    "top_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "for node, score in top_nodes:\n",
    "    lat, lon = G.nodes[node]['lat'], G.nodes[node]['lon']\n",
    "    size = scale(score, min(pagerank.values()), max(pagerank.values()), 3, 10)\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=size,\n",
    "        color='#DAA520',\n",
    "        fill=True,\n",
    "        fill_opacity=0.8,\n",
    "    ).add_to(network_map)\n",
    "\n",
    "network_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794c54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e3db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a28275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e0dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8e348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2725b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad50c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627188b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f45e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad0b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0b720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3d5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3876cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc880e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18493d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c31671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,10))\n",
    "# trips[\"hour\"] = pd.to_datetime(trips[\"started_at\"]).dt.hour\n",
    "# sns.lineplot(data=trips[trips['slope_type']!=\"flat\"], x=\"hour\", y=\"duration\", hue=\"slope_type\", estimator=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "# trips[\"weekday\"] = pd.to_datetime(trips[\"started_at\"]).dt.day_name()\n",
    "# sns.countplot(data=trips, x=\"weekday\", order=weekday_order)\n",
    "# plt.title(\"Number of Trips by Weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c912e6",
   "metadata": {},
   "source": [
    "Most people assume weekends are peak for casual biking, but data shows otherwise. Turns out, weekday commuting is the real engine behind Bysykkel usage  — not leisure rides.\n",
    "\n",
    "⸻  \n",
    "\n",
    "Quick Summary :  \n",
    "\t•\tSunday = lowest ride count  \n",
    "\t•\tSaturday = next lowest  \n",
    "\t•\tWeekdays = consistently higher  \n",
    "\t•\tLikely interpretation: Bysykkel is primarily a commuter tool, not just a weekend toy.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7caba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(data=trips, x=\"weekday\", y=\"duration\", order=weekday_order, estimator=\"mean\")\n",
    "# plt.title(\"Average Trip Duration by Weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fa136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(data=trips, x=\"weekday\", y=\"distance\", order=weekday_order, estimator=\"sum\")\n",
    "# plt.title(\"Total Trip Duration by Weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c816b3c",
   "metadata": {},
   "source": [
    "🚴‍♂️ Weekends = fewer trips, but longer ones  \n",
    "💼 Weekdays = more trips, but shorter  \n",
    "\n",
    "What that suggests:  \n",
    "\t•\tWeekdays: People are riding to work, school, or errands — short, practical, repeatable trips.  \n",
    "\t•\tWeekends: People ride for leisure — longer joyrides, detours, exploration, maybe even group rides.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b76ba",
   "metadata": {},
   "source": [
    "#con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
