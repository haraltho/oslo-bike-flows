{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49ed6fa",
   "metadata": {},
   "source": [
    "# Oslo Bike Share System Analysis\n",
    "\n",
    "Oslo's bike-sharing system, *Bysykkel*, is a popular way to get around the city. Historical ride data is freely available — a paradise for any data scientist. In this project, I explore how topography influences bike movement patterns, identify critical stations in the network, and examine rebalancing needs and temporal usage dynamics.\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "To discover actionable insights about Oslo’s bike-sharing system by analyzing patterns driven by topography, network structure, and temporal dynamics — with the aim of supporting operational improvements and user experience.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. **How does topography affect the flow of bikes?**\n",
    "2. **Which stations are most important to the network?**\n",
    "3. **What temporal patterns drive rebalancing needs?**\n",
    "4. **How can the system be optimized for better efficiency and reliability?**\n",
    "\n",
    "**Data Source:** [Oslo Bysykkel Historical Data](https://oslobysykkel.no/apne-data/historisk) (using all data from 2024)\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "1. **Data Exploration**\n",
    "2. **Cleaning in SQL**\n",
    "3. **Topographical Flow Analysis**\n",
    "4. **Network Structure Analysis**\n",
    "5. **Temporal Flow Analysis**\n",
    "6. **Rebalancing Insights & Optimization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import glob\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "import seaborn as sns\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c610395",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "In this phase, we'll explore the dataset in order to find data quality issues. The findings will be used to determine what cleaning steps are needed.\n",
    " \n",
    "### 1.1 Loading the data\n",
    "This step reads all monthly CSV files from the `../data/` folder and loads them into a DuckDB database file (`db/bysykkel_2024.duckdb`). If the database doesn't exist, it will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSVs into DuckDB table 'trips_raw'\n",
    "con = duckdb.connect(\"../db/bysykkel_2024.duckdb\")\n",
    "\n",
    "csv_files = glob.glob(\"../data/??.csv\")\n",
    "for i, file in enumerate(csv_files):\n",
    "    if i == 0:\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE trips_raw AS SELECT * FROM read_csv_auto('{file}')\")\n",
    "    else:\n",
    "        con.execute(f\"INSERT INTO trips_raw SELECT * FROM read_csv_auto('{file}')\")\n",
    "\n",
    "con.execute(\"CHECKPOINT\")\n",
    "print(\"Loaded trips_raw data into DuckDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896a7df",
   "metadata": {},
   "source": [
    "### 1.2 Exploring Dataset and Columns\n",
    "In this step we'll explore all the columns to get familiar with the dataset and detect data quality issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d50a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = con.execute('SELECT * FROM trips_raw').df()\n",
    "trips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb9732",
   "metadata": {},
   "source": [
    "The dataset contains 13 columns, where each column represents one ride. Only the `start_station_description`and the `end_station_description` contain null values. These columns contain no value to the project and will be discarded.  \n",
    "  \n",
    "❗ Drop start_station_description and end_station_description columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b0447",
   "metadata": {},
   "source": [
    "#### 1.2.1 started_at & ended_at\n",
    "These columns state the times at which a ride was started and ended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Check if ended_at is always after started_at\n",
    "(trips['ended_at'] >= trips['started_at']).all()\n",
    "print(f\"Are all return times after the trip was started? {(trips['ended_at'] >= trips['started_at']).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbda590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot usage frequency\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Group by day\n",
    "daily = trips.groupby(trips['started_at'].dt.date).size()\n",
    "\n",
    "# Convert index to datetime (from date)\n",
    "daily.index = pd.to_datetime(daily.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(daily.index, daily.values, color='royalblue')\n",
    "\n",
    "# Format x-axis with months\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "plt.title(\"Daily Ride Count (Binned by Day)\")\n",
    "plt.xlabel(\"Month [2024]\")\n",
    "plt.ylabel(\"Number of Rides\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802298e0",
   "metadata": {},
   "source": [
    "✅ No problems detected in this column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10326732",
   "metadata": {},
   "source": [
    "#### 1.2.2 Duration\n",
    "The `duration` column states the duration of the ride in seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf6800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot duration distribution, log scale helps\n",
    "plt.figure()\n",
    "sns.histplot(data=trips, x='duration', bins=100, log_scale=True)\n",
    "plt.title(\"Ride Duration: Loops vs. Non-Loops\")\n",
    "plt.xlabel(\"Ride Duration [sec]\")\n",
    "plt.axvline(7200, c='grey', linewidth=0.5, linestyle='--', label=\"2h\")\n",
    "plt.legend(frameon=False)\n",
    "plt.ylabel(\"Number of Rides\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "There are {(trips[trips['duration']>7200]).size} rides that exceed 2 hours. It is likely that these are not actual rides but bikes that were unsuccessfully returned. We will remove these from the dataset.  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba57aaa",
   "metadata": {},
   "source": [
    "✅ No missing values.  \n",
    "❗ Remove rides longer than 2 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6d685",
   "metadata": {},
   "source": [
    "#### 1.2.3 start_station_id & end_station_id\n",
    "These columns state a unique id for each station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35319b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of start stations: {len(set(trips['start_station_id']))}\")\n",
    "print(f\"Number of start stations: {len(set(trips['end_station_id']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ids = set(trips['start_station_id'].unique())\n",
    "end_ids = set(trips['end_station_id'].unique())\n",
    "\n",
    "only_start = start_ids - end_ids\n",
    "only_end = end_ids - start_ids\n",
    "\n",
    "print(f\"Start-only stations: {len(only_start)}\")\n",
    "print(f\"End-only stations: {len(only_end)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7867546",
   "metadata": {},
   "source": [
    "Check how many trips are **loops**, so trips where the start and end stations are identical. These might distort the picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(trips[trips['start_station_id']==trips['end_station_id']])} trips with identical start and end point. These will be discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970454f6",
   "metadata": {},
   "source": [
    "✅ No missing values.  \n",
    "❗ Remove loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40c4cc",
   "metadata": {},
   "source": [
    "#### 1.2.3 start_station_name & end_station_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bca24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of start stations: {len(set(trips['start_station_name']))}\")\n",
    "print(f\"Number of start stations: {len(set(trips['end_station_name']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e23a4c",
   "metadata": {},
   "source": [
    "There are two more unique station names than station id's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_names = set(trips['start_station_name'].unique())\n",
    "end_names = set(trips['end_station_name'].unique())\n",
    "\n",
    "only_start_names = start_names - end_names\n",
    "only_end_names = end_names - start_names\n",
    "\n",
    "print(f\"Start-only station names: {len(only_start_names)}\")\n",
    "print(f\"End-only station names: {len(only_end_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_name_map = (\n",
    "    trips[['start_station_id', 'start_station_name']]\n",
    "    .drop_duplicates()\n",
    "    .groupby('start_station_id')['start_station_name']\n",
    "    .nunique()\n",
    ")\n",
    "\n",
    "# IDs with >1 unique name\n",
    "inconsistent_ids = start_name_map[start_name_map > 1]\n",
    "print(f\"Start station IDs with multiple names: {len(inconsistent_ids)}\")\n",
    "print(inconsistent_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59429188",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_name_map = (\n",
    "    trips[['end_station_id', 'end_station_name']]\n",
    "    .drop_duplicates()\n",
    "    .groupby('end_station_id')['end_station_name']\n",
    "    .nunique()\n",
    ")\n",
    "\n",
    "# IDs with >1 unique name\n",
    "inconsistent_ids = end_name_map[end_name_map > 1]\n",
    "print(f\"End station IDs with multiple names: {len(inconsistent_ids)}\")\n",
    "print(inconsistent_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trips[trips['start_station_id']==608]\n",
    "df['start_station_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a58171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trips[trips['start_station_id']==1101]\n",
    "df['start_station_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c700f",
   "metadata": {},
   "source": [
    "❗ station_id 608 and 1101 don't have unique names. There is a small subset (~1%) of alternative names. When creating a station table it is important to use the station_id as the unique identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17734cc",
   "metadata": {},
   "source": [
    "#### 1.2.4 start_station_latitude, end_station_latitude, start_station_longitude, end_station_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479458d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "trips['start_station_latitude'].hist(ax=axs[0, 0], bins=50, color='skyblue')\n",
    "axs[0, 0].set_title('Start Station Latitude')\n",
    "\n",
    "trips['end_station_latitude'].hist(ax=axs[0, 1], bins=50, color='lightgreen')\n",
    "axs[0, 1].set_title('End Station Latitude')\n",
    "\n",
    "trips['start_station_longitude'].hist(ax=axs[1, 0], bins=50, color='skyblue')\n",
    "axs[1, 0].set_title('Start Station Longitude')\n",
    "\n",
    "trips['end_station_longitude'].hist(ax=axs[1, 1], bins=50, color='lightgreen')\n",
    "axs[1, 1].set_title('End Station Longitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89800c93",
   "metadata": {},
   "source": [
    "### Column Audit Summary\n",
    "\n",
    "| Column                      | Notes                                                                 |\n",
    "|-----------------------------|-----------------------------------------------------------------------|\n",
    "| `started_at`, `ended_at`    | ✅ Valid timestamps, no missing values. |\n",
    "| `duration`                  | ⚠️ No missing values, but contains outliers > 2 hours → to be removed in SQL. |\n",
    "| `start_station_id`          | ✅ Valid IDs. However, 30,188 trips are loops (start = end) → to be removed. |\n",
    "| `end_station_id`            | ✅ Valid IDs. Same note as above.                                    |\n",
    "| `start_station_name`        | ⚠️ Mostly consistent. Two IDs (608 and 1101) map to multiple names. Will use the most common. |\n",
    "| `end_station_name`          | ⚠️ Same as above. No major action needed if we trust IDs.            |\n",
    "| `start_station_description` | ❌ Incomplete. Will be dropped in cleaning step.                 |\n",
    "| `end_station_description`   | ❌ Same as above.                                                    |\n",
    "| `start_station_latitude`    | ✅ All values present. Range appears valid.                          |\n",
    "| `end_station_latitude`      | ✅ Same as above.                                                    |\n",
    "| `start_station_longitude`   | ✅ All values present. Range appears valid.                          |\n",
    "| `end_station_longitude`     | ✅ Same as above.                                                    |\n",
    "\n",
    "\n",
    "### Cleaning Actions to Apply in SQL\n",
    "\n",
    "- Remove trips longer than 2 hours  \n",
    "- Remove loops (start and end station ID are the same)  \n",
    "- Drop `start_station_description` and `end_station_description`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fdb56",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Cleaning in SQL\n",
    "In this step we'll clean the dataset and address the problems identified in our exploration phase.\n",
    "\n",
    "The goal is to create two tables:\n",
    "1. `trips_clean`: A filtered dataset without outliers and unnecessary columns, ready for analysis in pandas\n",
    "2. `stations`: A normalized table listing all stations in the system, containing consistent names, station IDs, and geographic coordinates\n",
    "\n",
    "### 2.1 trips_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70958a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rides longer than 2 hours (= 7200 sec)\n",
    "# Remove loops\n",
    "# Drop station descriptions\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE trips_clean AS\n",
    "SELECT \n",
    "    * EXCLUDE (start_station_description, end_station_description)\n",
    "FROM trips_raw\n",
    "WHERE\n",
    "    duration < 7200 AND\n",
    "    start_station_id != end_station_id\n",
    "\"\"\")\n",
    "con.execute(\"CHECKPOINT\")\n",
    "print(\"Cleaned trips saved to DuckDB as 'trips_clean'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b8ddd",
   "metadata": {},
   "source": [
    "### 2.2 Extract Unique Stations  \n",
    "Build a `stations` table by combining all distinct start and end stations from the `trips_clean` table. This gives the full list of physical bike stations to enrich with elevation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc295e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE stations AS\n",
    "SELECT DISTINCT\n",
    "    station_id,\n",
    "    station_name,\n",
    "    ROUND(lat, 5) AS lat,\n",
    "    ROUND(lon, 5) AS lon\n",
    "FROM (\n",
    "    SELECT\n",
    "        start_station_id AS station_id,\n",
    "        start_station_name AS station_name,\n",
    "        start_station_latitude AS lat,\n",
    "        start_station_longitude AS lon\n",
    "    FROM trips_clean\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT\n",
    "        end_station_id AS station_id,\n",
    "        end_station_name AS station_name,\n",
    "        end_station_latitude AS lat,\n",
    "        end_station_longitude AS lon\n",
    "    FROM trips_clean\n",
    ")\n",
    "ORDER BY station_id\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"CHECKPOINT\")\n",
    "print(\"Extracted and saved 'stations' table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99263f9f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Topographical Analysis\n",
    "In this phase of the project, we'll investigate whether Oslo's topography influences bike usage patterns. Oslo has a natural gradient from sea level upwards, which suggests that cyclists might prefer downhill over uphill routes.  \n",
    "\n",
    "**Questions we'll answer:**\n",
    "1. Do cyclists prefer downhill routes? If yes, by what margin?\n",
    "2. How does elevation affect station imbalance? Where does the net flux go?\n",
    "3. What stations are consistent exporters and importers of bikes? \n",
    "4. How do trip characteristics such as distance and duration differ between uphill and downhill rides?  \n",
    "\n",
    "### 3.1 Enhancing Data with Elevation Information\n",
    "Before analyzing the effect of the terrain, we'll need to add elevation information based on the geographic location of each bike station in the system. We do this using a free api service \"open-meteo.com\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stations table and add elevation data\n",
    "csv_path = \"../db/stations_with_elevation.csv\"\n",
    "\n",
    "def get_elevation(row):\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    \n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/elevation?latitude={lat}&longitude={lon}\")\n",
    "    data = response.json()\n",
    "    return data['elevation'][0]\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"Found existing data. Loading from csv.\")\n",
    "    stations = pd.read_csv(csv_path)\n",
    "else:\n",
    "    print(\"No csv found. Fetching elevation data from API.\")\n",
    "    stations = con.execute(\"SELECT * FROM stations\").df()\n",
    "    stations = stations.drop_duplicates(subset='station_id')\n",
    "    stations['elevation'] = stations.apply(get_elevation, axis=1)\n",
    "    stations.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Elevation received for all stations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save elevation data to database\n",
    "con.register(\"stations_df\", stations)\n",
    "con.execute(\"CREATE OR REPLACE TABLE stations AS SELECT * FROM stations_df\")\n",
    "con.unregister(\"stations_df\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join elevation data onto trips table\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE trips_clean AS\n",
    "SELECT\n",
    "    t.*,   \n",
    "    s_start.elevation AS start_elevation,\n",
    "    s_end.elevation AS end_elevation,\n",
    "    s_end.elevation - s_start.elevation AS elevation_diff\n",
    "FROM trips_clean t\n",
    "JOIN stations s_start ON t.start_station_id = s_start.station_id\n",
    "JOIN stations s_end ON t.end_station_id = s_end.station_id\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"CHECKPOINT\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78330ed",
   "metadata": {},
   "source": [
    "### 3.2 Loading Analysis-Ready Data into pandas\n",
    "Now that we have clean data with elevation information, we'll load it into pandas for the remainder of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enriched data into pandas for analysis\n",
    "trips = con.execute(\"SELECT * FROM trips_clean\").df()\n",
    "stations = con.execute(\"SELECT * FROM stations\").df()\n",
    "\n",
    "print(f\"Loaded {len(trips):,} clean trips and {len(stations)} stations\")\n",
    "print(f\"Data spans from {trips['started_at'].min()} to {trips['started_at'].max()}\")\n",
    "\n",
    "# Quick check of our elevation data\n",
    "print(f\"\\nElevation range: {stations['elevation'].min():.1f}m to {stations['elevation'].max():.1f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32671b",
   "metadata": {},
   "source": [
    "### 3.3 Calculating Trip Distance and Gradient\n",
    "To understand cyclist preferences, we need to calculate the gradient (slope) and distance of each trip."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
